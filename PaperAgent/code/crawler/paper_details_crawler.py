"""
Function: ä»paper_listçˆ¬å–æ¯ç¯‡è®ºæ–‡çš„è¯¦ç»†ä¿¡æ¯

CreateDay: 20250625
Author: HongfengAi
History:
20250625    HongfengAi  ç¬¬ä¸€ç‰ˆ
"""
import json
import time
import os
import sys
import requests
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import re
from datetime import datetime
# æ·»åŠ PDFå¤„ç†ç›¸å…³å¯¼å…¥
import PyPDF2 # pip install PyPDF2
import fitz  # pip install pymupdf
from PIL import Image
import io
from pdf2image import convert_from_path
from transformers import AutoProcessor, AutoModelForCausalLM
import torch


sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
from configs import USER_AGENT, PAPER_EXPRESS_ROOT_PATH
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from crawler.crawler_utils import open_browser
from utils import set_proxy, unset_proxy, safe_title_func
from wechat_utils.upload_material import WeChatPermanentMaterialUploader


class PaperDetailsCrawler:
    def __init__(self):
        self.browser = None
        self.wait = None
        self.media_uploader = WeChatPermanentMaterialUploader()
        

    def open_browser(self):
        """æ‰“å¼€æµè§ˆå™¨"""
        self.browser, self.wait = open_browser()
        

    def close_browser(self):
        """å…³é—­æµè§ˆå™¨"""
        if self.browser:
            self.browser.quit()
    

    def extract_github_url_from_pdf(self, pdf_url):
        """
        ä»PDFç¬¬ä¸€é¡µæå–GitHub URL
        
        Args:
            pdf_url: PDFæ–‡ä»¶çš„URL
            
        Returns:
            str or None: æ‰¾åˆ°çš„GitHub URLï¼Œå¦‚æœæ²¡æ‰¾åˆ°åˆ™è¿”å›None
        """
            
        try:
            print(f"ğŸ” æ­£åœ¨ä»PDFç¬¬ä¸€é¡µæå–GitHub URL: {pdf_url}")
            
            # ä¸‹è½½PDFæ–‡ä»¶
            headers = {
                'User-Agent': USER_AGENT
            }
            response = requests.get(pdf_url, headers=headers, timeout=30, stream=True)
            response.raise_for_status()
            
            # å°†PDFå†…å®¹å†™å…¥ä¸´æ—¶æ–‡ä»¶
            temp_pdf_path = "temp_paper.pdf"
            with open(temp_pdf_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            text_content = ""
            
            # å°è¯•ä½¿ç”¨PyPDF2
            try:
                with open(temp_pdf_path, 'rb') as pdf_file:
                    pdf_reader = PyPDF2.PdfReader(pdf_file)
                    if len(pdf_reader.pages) > 0:
                        text_content = pdf_reader.pages[0].extract_text()
            except Exception as e:
                print(f"âŒ PDFæ–‡æœ¬æå–å¤±è´¥: {e}")
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                if os.path.exists(temp_pdf_path):
                    os.remove(temp_pdf_path)
                return None
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(temp_pdf_path):
                os.remove(temp_pdf_path)
            
            if not text_content:
                print("âŒ æ— æ³•ä»PDFç¬¬ä¸€é¡µæå–æ–‡æœ¬å†…å®¹")
                return None
            
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æŸ¥æ‰¾GitHub URL
            github_patterns = [
                r'https?://github\.com/[A-Za-z0-9_.-]+/[A-Za-z0-9_.-]+',
                r'github\.com/[A-Za-z0-9_.-]+/[A-Za-z0-9_.-]+',
                r'https?://www\.github\.com/[A-Za-z0-9_.-]+/[A-Za-z0-9_.-]+'
            ]
            
            for pattern in github_patterns:
                matches = re.findall(pattern, text_content, re.IGNORECASE)
                if matches:
                    # ç¡®ä¿URLæ˜¯å®Œæ•´çš„
                    github_url = matches[0]
                    if not github_url.startswith('http'):
                        github_url = 'https://' + github_url
                    
                    print(f"âœ… ä»PDFç¬¬ä¸€é¡µæˆåŠŸæå–GitHub URL: {github_url}")
                    return github_url
            
            print("âŒ PDFç¬¬ä¸€é¡µä¸­æœªæ‰¾åˆ°GitHub URL")
            return None
            
        except requests.RequestException as e:
            print(f"âŒ ä¸‹è½½PDFæ–‡ä»¶å¤±è´¥: {e}")
            return None
        except Exception as e:
            print(f"âŒ ä»PDFæå–GitHub URLæ—¶å‘ç”Ÿé”™è¯¯: {e}")
            return None

    def extract_images_from_pdf(self, pdf_url, save_dir, max_images=5):
        """
        ä½¿ç”¨TF-IDæ¨¡å‹ä»PDFä¸­æŠ½å–è¡¨æ ¼/å›¾ç‰‡ï¼ŒåªæŠ½å–max_imageså¼ ï¼Œä¸Šä¼ åˆ°å¾®ä¿¡ç´ æåº“ï¼Œè¿”å›image urlåˆ—è¡¨ã€‚
        """
        try:
            print(f"ğŸ” æ­£åœ¨ä»PDFä¸­æŠ½å–è¡¨æ ¼/å›¾ç‰‡: {pdf_url}")
            set_proxy()
            headers = {'User-Agent': USER_AGENT}
            response = requests.get(pdf_url, headers=headers, timeout=60)
            response.raise_for_status()
            # ä¿å­˜ä¸´æ—¶PDFæ–‡ä»¶
            os.makedirs(save_dir, exist_ok=True)
            temp_pdf_path = os.path.join(save_dir, "temp_paper.pdf")
            with open(temp_pdf_path, 'wb') as f:
                f.write(response.content)

            # ===== TF-IDæ¨¡å‹æ¨ç†éƒ¨åˆ† =====
            model_id = "yifeihu/TF-ID-base"
            images = convert_from_path(temp_pdf_path)
            model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True)
            processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)
            print("Model loaded: ", model_id)
            unset_proxy()
            
            img_urls = []
            img_count = 0
            for i, image in enumerate(images):
                # æ¨ç†
                prompt = "<OD>"
                inputs = processor(text=prompt, images=image, return_tensors="pt")
                with torch.no_grad():
                    generated_ids = model.generate(
                        input_ids=inputs["input_ids"],
                        pixel_values=inputs["pixel_values"],
                        max_new_tokens=1024,
                        do_sample=False,
                        num_beams=3
                    )
                generated_text = processor.batch_decode(generated_ids, skip_special_tokens=False)[0]
                annotation = processor.post_process_generation(generated_text, task="<OD>", image_size=(image.width, image.height))["<OD>"]
                # è£å‰ªå¹¶ä¿å­˜å›¾ç‰‡
                for j, bbox in enumerate(annotation['bboxes']):
                    label = annotation['labels'][j]
                    x1, y1, x2, y2 = bbox
                    cropped_image = image.crop((x1, y1, x2, y2))
                    img_path = os.path.join(save_dir, f"page_{i}_{label}_{j}.png")
                    cropped_image.save(img_path)
                    # ä¸Šä¼ åˆ°å¾®ä¿¡
                    upload_result = self.media_uploader.upload_specific_file(img_path)
                    if upload_result and upload_result.get('success') and upload_result.get('url'):
                        img_urls.append(upload_result['url'])
                    img_count += 1
                    if img_count >= max_images:
                        break
                if img_count >= max_images:
                    break
            # æ¸…ç†ä¸´æ—¶PDF
            if os.path.exists(temp_pdf_path):
                os.remove(temp_pdf_path)
            print(f"âœ“ æˆåŠŸæŠ½å–å¹¶ä¸Šä¼  {len(img_urls)} å¼ å›¾ç‰‡")
            return img_urls
        except Exception as e:
            print(f"âŒ PDFå›¾ç‰‡æŠ½å–å¤±è´¥: {e}")
            return []

    def extract_paper_details(self, hf_url):
        """
        ä»Hugging Faceè®ºæ–‡é¡µé¢æå–è¯¦ç»†ä¿¡æ¯
        
        Args:
            hf_url: è®ºæ–‡åœ¨HFä¸Šçš„URL
            
        Returns:
            dict: è®ºæ–‡è¯¦ç»†ä¿¡æ¯
        """
        # è®¿é—®è®ºæ–‡é¡µé¢
        print(f"æ­£åœ¨è®¿é—®è®ºæ–‡é¡µé¢: {hf_url}")
        self.browser.get(hf_url)
        
        # ç­‰å¾…é¡µé¢åŠ è½½
        time.sleep(3)
        
        paper_details = {}
        
        # æå–è®ºæ–‡å‘è¡¨æ—¶é—´
        date_element = self.browser.find_element(By.XPATH, "/html/body/div[1]/main/div/section[1]/div/div[1]/div[2]/div[1]")
        paper_details['published_date'] = date_element.text

        # æå–ä½œè€…åˆ—è¡¨
        author_elements = self.browser.find_elements(By.XPATH, "/html/body/div[1]/main/div/section[1]/div/div[1]/div[4]/*")
        authors = []
        for author_element in author_elements:
            if author_element.text == 'Authors:':
                pass
            else:
                authors.append(author_element.text.replace("\n,", ''))
        paper_details['authors'] = authors
        
        # æå–AIç”Ÿæˆæ€»ç»“
        try:
            ai_summary_element = self.browser.find_element(By.XPATH, "/html/body/div/main/div/section[1]/div/div[2]/div/div/p")
            paper_details['ai_summary'] = ai_summary_element.text
        except NoSuchElementException:
            paper_details['ai_summary'] = ""
            pass
        

        # æå–æ‘˜è¦
        abstract_element = self.browser.find_element(By.XPATH, "/html/body/div/main/div/section[1]/div/div[2]/div/p")
        paper_details['abstract'] = abstract_element.text

        # æå–PDFé“¾æ¥å’ŒarXivä¿¡æ¯
        link_elements = self.browser.find_elements(By.XPATH, "/html/body/div[1]/main/div/section[1]/div/div[3]/*")
        paper_details['pdf_url'] = None
        paper_details['github_url'] = None
        for link_element in link_elements:
            if link_element.text == 'View PDF' or link_element.text == 'View arXiv page':
                paper_details['pdf_url'] = link_element.get_attribute('href')
            elif 'GitHub' in link_element.text:
                paper_details['github_url'] = link_element.get_attribute('href')
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°GitHub URLä½†æœ‰PDF URLï¼Œå°è¯•ä»PDFç¬¬ä¸€é¡µæå–
        if paper_details['github_url'] is None and paper_details['pdf_url'] is not None:
            print("ğŸ” Hugging Faceé¡µé¢æœªæ‰¾åˆ°GitHub URLï¼Œå°è¯•ä»PDFç¬¬ä¸€é¡µæå–...")
            pdf_github_url = self.extract_github_url_from_pdf(paper_details['pdf_url'])
            if pdf_github_url:
                paper_details['github_url'] = pdf_github_url
            else:
                print("âŒ PDFç¬¬ä¸€é¡µä¹Ÿæœªæ‰¾åˆ°GitHub URL")
        
        return paper_details


    def crawl_papers_details_from_list(self, paper_list_file, output_dir, max_papers=None):
        """
        ä»è®ºæ–‡åˆ—è¡¨æ–‡ä»¶ä¸­è¯»å–è®ºæ–‡ä¿¡æ¯å¹¶çˆ¬å–è¯¦æƒ…
        
        Args:
            paper_list_file: è®ºæ–‡åˆ—è¡¨JSONæ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            max_papers: æœ€å¤§çˆ¬å–è®ºæ–‡æ•°é‡ï¼ŒNoneè¡¨ç¤ºçˆ¬å–æ‰€æœ‰
            
        Returns:
            None
        """
        # è¯»å–è®ºæ–‡åˆ—è¡¨
        with open(paper_list_file, 'r', encoding='utf-8') as f:
            papers_list = json.load(f)
        
        print(f"æ‰¾åˆ° {len(papers_list)} ç¯‡è®ºæ–‡")
        
        # é™åˆ¶çˆ¬å–æ•°é‡
        if max_papers:
            papers_list = papers_list[:max_papers]
            print(f"é™åˆ¶çˆ¬å–å‰ {max_papers} ç¯‡è®ºæ–‡")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        crawled_papers = []
        
        for i, paper in enumerate(papers_list):
            print(f"\næ­£åœ¨å¤„ç†ç¬¬ {i+1}/{len(papers_list)} ç¯‡è®ºæ–‡: {paper.get('title', 'Unknown')}")

            # æ‰“å¼€æµè§ˆå™¨
            self.open_browser()

            # çˆ¬å–è®ºæ–‡è¯¦æƒ…
            paper_details = self.extract_paper_details(paper['hf_url'])

            # å…³é—­æµè§ˆå™¨
            self.close_browser()
            
            # åˆå¹¶åŸºæœ¬ä¿¡æ¯å’Œè¯¦ç»†ä¿¡æ¯
            combined_paper_info = {
                **paper,  # åŸæœ‰çš„åŸºæœ¬ä¿¡æ¯
                **paper_details  # æ–°çˆ¬å–çš„è¯¦ç»†ä¿¡æ¯
            }
            
            # æ–°å¢ï¼šæŠ½å–è®ºæ–‡å›¾ç‰‡å¹¶ä¸Šä¼ 
            paper_img_urls = []
            if combined_paper_info.get('pdf_url'):
                safe_title = safe_title_func(paper.get('title', f'{i+1}_paper'))
                paper_img_dir = os.path.join(output_dir, f"{i+1}_{safe_title}", "paper_imgs")
                paper_img_urls = self.extract_images_from_pdf(combined_paper_info['pdf_url'], paper_img_dir, max_images=5)
            combined_paper_info['paper_img_urls'] = paper_img_urls
            crawled_papers.append(combined_paper_info)
            
            # ä¸ºæ¯ç¯‡è®ºæ–‡åˆ›å»ºå•ç‹¬çš„æ–‡ä»¶
            safe_title = safe_title_func(paper.get('title', f'{i+1}_paper'))
            paper_path = os.path.join(output_dir, f"{i+1}_{safe_title}")
            paper_file = os.path.join(paper_path, f"paper_details.json")
            
            with open(paper_file, 'w', encoding='utf-8') as f:
                json.dump(combined_paper_info, f, ensure_ascii=False, indent=2)
            
            print(f"å·²ä¿å­˜è®ºæ–‡è¯¦æƒ…åˆ°: {paper_file}")
            
            # é¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(2)
        
        # ä¿å­˜æ‰€æœ‰è®ºæ–‡çš„è¯¦ç»†ä¿¡æ¯æ±‡æ€»
        summary_file = os.path.join(output_dir, "all_papers_details.json")
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(crawled_papers, f, ensure_ascii=False, indent=2)
        
        print(f"\nçˆ¬å–å®Œæˆï¼å…±å¤„ç† {len(crawled_papers)} ç¯‡è®ºæ–‡")
        print(f"è¯¦ç»†ä¿¡æ¯å·²ä¿å­˜åˆ°: {summary_file}")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='è®ºæ–‡è¯¦æƒ…çˆ¬è™«')
    parser.add_argument('--paper_list', type=str, required=True, default='C:/Users/alvin/Downloads/paper_express/2025_W25/paper_list_2025-W25.json', help='è®ºæ–‡åˆ—è¡¨JSONæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output_dir', type=str, required=True, default='C:/Users/alvin/Downloads/paper_express/2025_W25', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--max_papers', type=int, required=True, default=10, help='æœ€å¤§çˆ¬å–è®ºæ–‡æ•°é‡')
    
    args = parser.parse_args()
    
    crawler = PaperDetailsCrawler()
    crawler.crawl_papers_details_from_list(
        paper_list_file=args.paper_list,
        output_dir=args.output_dir,
        max_papers=args.max_papers
    )

